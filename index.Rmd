---
title: "Computational Musicology 2020-2021"
author: "Jim Buissink"
#date: "2/9/2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(spotifyr)
library(tidyverse)
library(knitr)
library(ggthemes)
library(plotly)
library(compmus)
library(ggplot2)
library(grid)
library(gridExtra)
library(tidymodels)
library(ggdendro)
library(heatmaply)
library(dendextend)
```


### Introduction

> I miss the old Kanye... 
> "I Love Kanye" - Kanye West (2016)

But who, what & when is this 'old' Kanye? Is it possible to uncover who the old Kanye is by analyzing his music? Are there any significant differences in his earlier and later works, and is there a way to cluster these works so we have an 'old' Kanye and a 'new' Kanye? 

To find an answer to these questions, we will build a corpus consisting of the 9 studio albums released by Kanye West and his 2 collaborative albums. Any non-musical tracks, e.g. skits, featured on the albums will be included. If a distinction can be made between old Kanye and new Kanye, we expect there to be musical differences between his first three albums and his latter releases. This is where the music changed from cheery & soulful to dark & electronic-inspired, so we expect to see differences in valence as this indicates mood and timbre coefficients as these indicate the tone color. His latest album, Jesus Is King (2019), is an outlier concerning this stylistic change and could mean that Kanye West is returning to his original sound, or that potentially a 'future' Kanye is in the making.

The 2 collaborative albums (with Jay-Z and Kid Cudi, respectively) are included as I feel that Kanye did have enough of an influence during the creation of these works so his personality should be reflected in the music, although it might be for only 50%. Singles and tracks where Kanye was a featured artist are excluded from the corpus.

First, we will try to find some useful features we could use for our clustering algorithm using Spotify's API. Afterwards, we will investigate other tools to compare musical pieces and finally we will implement our clustering algorithm which will be used to discover which albums sound the most similar.



### Seperating albums on their mood using **valence** 
```{r, echo=FALSE}
kanye_albums <- get_playlist_audio_features("", "4bQNKK5gntETwqlI1RW9w1")


#ADD MEAN FEATURES
kanye_albums  <- transform(kanye_albums,
    mean_tempo = mean(tempo),
    mean_valence = mean(valence),
    mean_speechiness = mean(speechiness),
    mean_energy = mean(energy)

)


#ADD POSITIVE COLUMN
kanye_albums <- kanye_albums %>% add_column(mood = 'yes')
kanye_albums <- kanye_albums %>% 
  mutate(mood= ifelse(track.album.name == 'The College Dropout' | track.album.name == 'Graduation' | track.album.name == 'Late Registration', "Positive", "Negative"))

#ADD AVG Tempo Column
kanye_albums <- kanye_albums %>% 
  mutate(avg_tempo= ifelse(track.album.name == 'The College Dropout' | track.album.name == 'JESUS IS KING' | track.album.name == 'Graduation' | track.album.name == 'Late Registration', "< 100 bpm", "> 100 bpm"))


```

```{r, echo=FALSE}

#BOXPLOTS 1
p <- ggplot(kanye_albums, aes(x = reorder(track.album.release_date, desc(track.album.release_date)), y = valence, fill=mood)) + 
  geom_boxplot() 
p <- p + scale_x_discrete(labels=c("JESUS IS KING","KIDS SEE GHOSTS","ye","The Life Of Pablo", "Yeezus", "Watch The Throne", "My Beautiful Dark Twisted Fantasy", "808s & Heartbreak", "Graduation", "Late Registration", "The College Dropout"))
p <- p +  geom_hline(yintercept = 0.40, linetype = 3)
p <- p + coord_flip() + labs(title="Distribution of the valence feature",y="", x = "")

p <- p + annotate(
  "text",
  x = 2.5, y = 0.58,
  label = "Average\nvalence",
  vjust = 1, size = 3, color = "grey40"
)

p <- p + annotate(
  "curve",
  x = 2.5, y = 0.58,
  xend = 2.5, yend = 0.40,
  arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
  color = "grey40")
p <- p + theme(legend.title=element_blank())
p #+ theme_economist_white()
```

***

As mentioned in the introduction, we need to find out which audio features contain the most useful information for our corpus.

One of such features is **valence**, which Spotify describes as the **musical positiveness**. To the left is a plot of the distribution of the valence for each album. We can see that his first three albums sound positive (valence > 0.5) and the latter sound negative (valence < 0.5). For this reason, valence seems to be a good feature as there is a clear distinction between Kanye's early and newer work.

### Seperating albums on their **tempo** using BPM 
```{r, echo=FALSE}
#BOXPLOTS 2
p2 <- ggplot(kanye_albums, aes(x = reorder(track.album.release_date, desc(track.album.release_date)), y = tempo, fill=avg_tempo)) + 
  geom_boxplot() 
p2 <- p2 + scale_x_discrete(labels=c("JESUS IS KING","KIDS SEE GHOSTS","ye","The Life Of Pablo", "Yeezus", "Watch The Throne", "My Beautiful Dark Twisted Fantasy", "808s & Heartbreak", "Graduation", "Late Registration", "The College Dropout"))
p2 <- p2 +  geom_hline(yintercept = 112, linetype = 3)
p2 <- p2 +  coord_flip() + labs(title="Distribution of the tempo feature",y="", x = "")

p2 <- p2 + annotate(
  "text",
  x = 10, y = 135,
  label = "Average\ntempo",
  vjust = 1, size = 3, color = "grey40"
)

p2 <- p2 + annotate(
  "curve",
  x = 10, y = 135,
  xend = 9, yend = 112,
  arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
  color = "grey40")

p2 <- p2 + theme(legend.title=element_blank())
p2

```

***

Another feature with similar behavior is the tempo. From the Spotify API, we can find the tempo in **beats per minute (BPM)** for each track. From the plot of the distribution of these tempi, we see that again the first three albums are distinct from the rest of Kanye's work. There is one exception for his latest album 'JESUS IS KING. These albums all have an **average BPM of < 100** whilst the rest of his albums all have an **average BPM of > 100**

The BPM used for hip-hop beats is generally around 60-100 bpm. This indicates that his early and newest work are typical Hip Hop albums but the other albums aren't.


### Seperating albums on their **emotional level** using valence and energy
```{r , echo=FALSE}
mood <- kanye_albums %>% ggplot(aes(x = valence, y = energy, color = track.album.release_date)) +
  geom_jitter(alpha = 0.6) + geom_hline(yintercept = 0.5) + geom_vline(xintercept = 0.5) 

mood <- mood + labs(title="Distribution of tracks according to valence and energy", colour = "Release date")

mood <- mood + annotate(
  "text",
  x = 0.90, y = 0.95,
  label = "HAPPY",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.1, y = 0.95,
  label = "ANGRY",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.1, y = 0.1,
  label = "SAD",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.90, y = 0.1,
  label = "CALM",
  vjust = 1, size = 4
)
fig <- ggplotly(mood)

fig
```

***

Whereas valence as a standalone feature tells us something about the general mood of a song, e.g. positive or negative, we could combine the valence with the energy feature. This pair of parameters now tells us something about the emotional level of a track. Low energy + low valence songs are considered 'sad', low valence + high energy are 'angry', songs with high valence + low energy are 'calm' and high valence + high energy are 'happy'.

Using this new **emotion** feature, we can see that most of Kanye's **early** work is classified as **Happy**, his **newest** work is mostly **Sad** and the albums released **in between** are **Angry**.

### Using Spotifyâ€™s **lower-level track audio features** to find differences in Kanye's albums

```{r, echo = FALSE}
YEE <- get_playlist_audio_features(
    "Jim Buissink",
    "2CcGtxKk4k5EbZC53tCBTr"
  ) %>%
  add_audio_analysis()

TCD <- get_playlist_audio_features(
    "Jim Buissink",
    "388Cb240QQLt1i5P9iVH6j"
  ) %>%
  #slice(2:13) %>%
  add_audio_analysis()

album123 <-
  YEE %>%
  mutate(album = "YEEZUS") %>%
  bind_rows(TCD %>% mutate(album = "The College Dropout"))
  
```

```{r, echo=FALSE}
# EOEs <- get_playlist_audio_features(
#     "Jim Buissink",
#     "3feALye5YioFPOoV6d7LrP"
#   ) %>%
#   slice(1:11) %>%
#   add_audio_analysis()
# 
# JIK <- get_playlist_audio_features(
#     "Jim Buissink",
#     "6LDcbbYnHVHlfPYUbRBY1H"
#   ) %>%
#   add_audio_analysis()

```

```{r, echo=FALSE}
plottt1 <- album123 %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(album, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = album)) +
  geom_boxplot(outlier.shape = NA)  + theme(legend.position = c(0.70, 0.8),
          legend.direction = "horizontal") +
  #scale_fill_viridis_d() +
  labs(x = "Timbre Coefficients", y = "", fill = "Album")
# 
# plottt12 <- album123 %>%
#   mutate(
#     timbre =
#       map(
#         segments,
#         compmus_summarise,
#         timbre,
#         method = "mean"
#       )
#   ) %>%
#   select(album, timbre) %>%
#   compmus_gather_timbre() %>%
#   ggplot(aes(x = basis, y = value, fill = album)) +
#   geom_violin() + theme(legend.position = "none") +
#   #scale_fill_viridis_d() +
#   labs(x = "Spotify Timbre Coefficients", y = "", fill = "Album")

```

```{r, echo=FALSE}
plottt2 <- album123 %>%
  mutate(
    pitches =
      map(
        segments,
        compmus_summarise,
        pitches,
        method = "rms", norm = "euclidean"
      )
  ) %>%
  select(album, pitches) %>%
  compmus_gather_chroma() %>%
  ggplot(aes(x = pitch_class, y = value, fill = album)) +
  geom_boxplot(outlier.shape = NA) + theme(legend.position = "none") +
  #scale_fill_viridis_d() +
  labs(x = "Pitch Classes", y = "", fill = "Album")
# 
# plottt22 <- album123 %>%
#   mutate(
#     pitches =
#       map(
#         segments,
#         compmus_summarise,
#         pitches,
#         method = "rms", norm = "euclidean"
#       )
#   ) %>%
#   select(album, pitches) %>%
#   compmus_gather_chroma() %>%
#   ggplot(aes(x = pitch_class, y = value, fill = album)) +
#   geom_violin()  +
#   labs(x = "Spotify Pitch Classes", y = "", fill = "Album")
```

```{r, echo=FALSE}
# 
# # Create two plots without legends
# plotttx <- album123 %>%
#   mutate(
#     pitches =
#       map(
#         segments,
#         compmus_summarise,
#         pitches,
#         method = "rms", norm = "euclidean"
#       )
#   ) %>%
#   select(album, pitches) %>%
#   compmus_gather_chroma() %>%
#   ggplot(aes(x = pitch_class, y = value, fill = album)) +
#   geom_boxplot(outlier.shape = NA) + 
#   #scale_fill_viridis_d() +
#   labs(x = "Spotify Pitch Classes", y = "", fill = "Album") +
#   theme(legend.position = "right", legend.box = "horizontal") 
# 
# # Create user-defined function, which extracts legends from ggplots
# extract_legend <- function(my_ggp) {
#   step1 <- ggplot_gtable(ggplot_build(my_ggp))
#   step2 <- which(sapply(step1$grobs, function(x) x$name) == "guide-box")
#   step3 <- step1$grobs[[step2]]
#   return(step3)
# }
# 
# # Apply user-defined function to extract legend
# shared_legend <- extract_legend(plotttx)

# Draw plots with shared legend 
grid.arrange(arrangeGrob(plottt1, plottt2, ncol = 1, top="Spotifyâ€™s lower-level track audio analysis"))
```

***

We already saw from the previous pages that we were able to distinguish Kanye's work using Spotify's audio features like valence, energy & tempo. These features are quite high-level, meaning that Spotify already did a lot of work to obtain these features. It is also possible to obtain lower level features from Spotify's API, which might also add extra information when trying to cluster albums into groups.

We are going to compare two albums, *Yeezus* & The *College Dropout (TCD)*. These albums are, according to analysis using the higher level features, the most contrasting works from Kanye's discography. For this comparison we will look at the **average timbre coefficients** and the **average pitch classes**. From the two distributions of these features, we can actually see that **these two albums are not as distinct as we might have thought**. 

For the **timbre coefficients** we see that they are **distributed similarly** for both albums. There is an exception for coefficient c05 where the mean is a bit higher for TCD compared to Yeezus, and there is one for coefficient c07 where the distribution for Yeezus is left-skewed, and vice versa. For the **pitch classes**, there is a bit **more distinction** between the two albums. **Especially the variance** of the pitch classes, which is significantly larger for Yeezus than for TDC, could potentially serve as an extra variable when trying to cluster Kanye's albums.

It seems that we could use these timbre coefficients and pitch classes as extra features for our clustering algrithm, because they could serve both as a way to group albums together and as a way to distinguish albums. 



### Can we use Spotify's **tempo feature** for **classification** or **clustering**?

```{r, echo=FALSE}
follow_god<-  get_tidy_audio_analysis("2QpGZOhTCHHiKmpSO9FW4h")
bound_2 <-  get_tidy_audio_analysis("3sNVsP50132BTNlImLx70i")
wonder <- get_tidy_audio_analysis("4jQqM4NI79HEcWHUJb8Hvf")

```


```{r, echo=FALSE, cache=TRUE}

a1 <- follow_god %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Follow God - Fourier-based") +
  theme_classic()

a2 <- follow_god %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Follow God - Cyclic") +
  theme_classic()

b1 <- wonder %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "I Wonder - Fourier-based") +
  theme_classic()

b2<- wonder %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "I Wonder - Cyclic") +
  theme_classic()
grid.arrange(arrangeGrob(b1, b2, a1, a2, ncol = 2, top="Tempograms of outliers"))
```

*** 

In our previous work (*Seperating albums on their tempo using BPM*), we identified Spotify's tempo feature as a possible candidate to distinguish Kanye's albums. We also identified a couple of outliers, which only occurred for his slower & early work (and his latest album, which indicated a new change of styles). We will inspect the two worst outliers using a tempogram. These outliers are *"I Wonder"* from the album *"Graduation"* and *"Follow God"* from the album *"Jesus Is King"*. 

For our first song, "I Wonder", Spotify labeled the tempo as **191.385 BPM**. This is actually the highest for our entire corpus, and differs greatly from the mean tempo of the album it is featured on which is 80 BPM. Listening to the track, we don't get the idea that this BPM is correct and neither that Spotify should have a hard time measuring the BPM. Upon inspecting the "Fourier-based" and "Cyclic" tempograms, we see that indeed Spotify was able to produce clear plots. However, it prefers to label the BPM according to the "Fourier-based" tempogram which picks up strongly on **tempo harmonics**. The correct BPM should be the sub-harmonic, **96 BPM**

We see the same behavior for our other outlier "Follow God". Spotify incorrectly labeled the BPM as **180 BPM**, with the correct being its sub-harmonic **90 BPM**. 

These **incorrect labels will hurt the performance of a classifying/clustering algorithm**, as these outliers are more likely to be assigned to the incorrect groups. Therefore, we should correct the tempo features for such outliers using tempograms before we apply our clustering algorithm .

### Comparing the **structure** of the opening tracks of My Beautiful Dark Twisted fantasy and Yeezus 

```{r, echo=FALSE}
dark_fantasy <-  get_tidy_audio_analysis("7yNK27ZTpHew0c55VvIJgm") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"              # Change summary & norm.
      )
  )


bind_rows(
  dark_fantasy%>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  dark_fantasy %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")

```

***

Another way to compare Kanye's music is to look at the way he structured his songs using **self-similarity matrices (SSMs)**. Lets have a look at the opening tracks of his 5th album My Beautiful Dark Twisted Fantasy (2010) and his 6th album Yeezus (2013). 

My Beautiful Dark Twisted Fantasy (MBDTF) features a maximalist aesthetic & clean production quality. Wikipedia classified the album as Hip hop / prog-rap/ popart/ pop / rap opera. If we think about most of these genres, we think of a clearly structured song. The opening track for MBDTF is self titled 'Dark Fantasy' and checks this box. 

From both the chroma and the timbre matrix, we can see the structure of the song. After a short intro, we start with a prolonged version of the chorus. From the diagonal lines in the chroma matrix, we can clearly see that this chorus itself is repeating a couple of bars. Following this we have two verses from Kanye with a shorter version of the chorus in between. After the 3rd chorus, we have a small bridge section and another short version of the chorus. The track then ends with an instrumental loop and after a brief silence + sharp sound (clearly depicted in the timbre matrix), we encounter the long version of the chorus one last time.


### Comparing the **structure** of the opening tracks of My Beautiful Dark Twisted fantasy and Yeezus **cont.**

```{r, echo = FALSE}
on_sight <-  get_tidy_audio_analysis("1gqkRc9WtOpnGIqxf2Hvzr") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"              # Change summary & norm.
      )
  )

bind_rows(
  on_sight%>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  on_sight %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")

) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***

Contrasting his previous release, Kanye's approach for Yeezus is in favor of a more minimalist approach. The album has been characterized as West's most experimental and sonically abrasive work. Because of the great reviews for his previous album, expectations were sky high for this release. But, instead of trying to one up his previous masterpiece he does a complete 180. Yeezus is a divisive album, and clearly does not fit in the 'Old Kanye' box. Lets have a look at the opening track 'On Sight'. 

It is very hard to see a clear structure from the matrices, and every line is very blurry. This could be because of the way the track is produced, very compressed and industrial sounding. In the chroma matrix we do see the interlude clearly, starting at around 75 seconds into the song, but is less visible in the timbre matrix. Vice versa, the outro which starts at around 120 seconds into the song is visible in the timbre matrix, but not in the chroma matrix.

This indicates that we could also distinguish Kanye's work using a different approach. But, as we are most interested in clustering his albums using a clustering algorithm, we choose to stick to the audio features mentioned in the previous tabs.



```{r, echo=FALSE}
# famous <- get_tidy_audio_analysis("19a3JfW8BQwqHWUMbcqSx8")
# 
# famous %>%
#   tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
#   ggplot(aes(x = time, y = bpm, fill = power)) +
#   geom_raster() +
#   scale_fill_viridis_c(guide = "none") +
#   labs(x = "Time (s)", y = "Tempo (BPM)", title = "Famous not cyclic") +
#   theme_classic()
# 
# famous %>%
#   tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
#   ggplot(aes(x = time, y = bpm, fill = power)) +
#   geom_raster() +
#   scale_fill_viridis_c(guide = "none") +
#   labs(x = "Time (s)", y = "Tempo (BPM)", title = "Famous cyclic") +
#   theme_classic()
```


### Clustering **'old'** & **'new'** Kanye using Spotify's selected high level features

```{r, echo=FALSE}
kanye_albums <- get_playlist_audio_features("", "4bQNKK5gntETwqlI1RW9w1")
kanye_albums$track.album.name[kanye_albums$track.album.name == 'The College Dropout'] <- ' (1) The College Dropout'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'Late Registration'] <- ' (2) Late Registration'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'Graduation'] <- ' (3) Graduation'
kanye_albums$track.album.name[kanye_albums$track.album.name == '808s & Heartbreak'] <- ' (4) 808s & Heartbreak'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'My Beautiful Dark Twisted Fantasy'] <- ' (5) My Beautiful Dark Twisted Fantasy'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'Watch The Throne'] <- ' (6) Watch The Throne'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'Yeezus'] <- ' (7) Yeezus'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'The Life Of Pablo'] <- ' (7) The Life Of Pablo'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'ye'] <- ' (8) ye'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'KIDS SEE GHOSTS'] <- ' (9) KIDS SEE GHOSTS'
kanye_albums$track.album.name[kanye_albums$track.album.name == 'JESUS IS KING'] <- ' (10) JESUS IS KING'
#kanye_albums <- kanye_albums %>% mutate(track.album.name = if(track.album.name == 'The College Dropout', 'The College Dropout')
cluster_test <- kanye_albums %>% group_by(track.album.name) %>%
 summarize(mean_tempo = mean(tempo), mean_energy = mean(energy), mean_valence = mean(valence), sd_tempo = sd(tempo), sd_energy = sd(energy), sd_valence = sd(valence))
```

```{r, echo=FALSE}
cluster1_juice <-
  recipe(
    track.album.name ~
    mean_tempo + mean_energy + mean_valence,
    data = cluster_test
  ) %>%
  #step_center(all_predictors()) %>%
  #step_scale(all_predictors()) %>% 
  step_range(all_predictors()) %>% 
  prep(cluster_test %>% mutate(track.album.name = str_trunc(track.album.name, 30))) %>%
  juice() %>%
  column_to_rownames("track.album.name")
```


```{r, echo=FALSE}
cluster1_dist <- dist(cluster1_juice, method = "manhattan")
```

```{r, echo=FALSE}
# dend <- cluster1_dist %>% hclust(method = "average") %>% as.dendrogram 
# dend %>% set("labels_col", value = c("green", "blue"), k=2) %>% set("branches_k_color", value = c("green", "blue"), k = 2) %>% as.ggdend(dend)
# ggplot(dend)
```


```{r, echo=FALSE}
cluster1 <- cluster1_dist %>% 
  hclust(method = "average") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram(rotate = TRUE) +
  labs(x = "", y = "", title = "Clustering albums on average valence, tempo & energy") 
cluster1
```

***

So, lets find out if there is any distinction between an *'old'* & *'new'* Kanye. From our previous work, we found that Spotify's low level features such as **tempo, valence & energy** looked like good variables for a clustering algorithm. Using the mean of these features for each album, (as we are interested in grouping albums together and not particular tracks), we create our clusters using hierarchical clustering which can be seen in the dendogram.

If we look at the top level, where we get two clusters, we find that indeed his early work is grouped together. But we also find his latest album, 'JESUS IS KING' in this cluster. This indicates that Kanye West is returning to his 'old' self, and that this term does not refer to a period in time, but rather a typical sound. 

There are some other interesting observations when looking at lower levels of the dendogram. For instance, his collaborative album 'Watch The Throne' is an outlier in the sense that it is part of a rather vague & large cluster 'new Kanye', but none other. This makes sense for the reason that it is not completely his own work and upon listening to the album, it also does not sound like a Kanye West album.

Other smaller sub-clusters from the 'new Kanye' clusters are:  

1. His emotional, sad albums: 'ye' + '808s and hearthbreak'
2. His experimental, industrial album: 'Yeezus'
3. His more poppy, less thematic albums: 'The Life of Pablo' + 'KIDS SEE GHOSTS' + 'My Beautiful Dark Twisted Fantasy'

<!-- Checken dat de grammy winning albums een cluster vormen. -->


### Clustering **'old'** & **'new'** Kanye using Spotify's selected high level and low level features

```{r, echo=FALSE}
cluster_test3 <- kanye_albums %>%  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre)) %>%
  group_by(track.album.name) %>%
  summarize(mean_tempo = mean(tempo), mean_energy = mean(energy), mean_valence = mean(valence), sd_tempo = sd(tempo), sd_energy = sd(energy), sd_valence = sd(valence), mean_c01 = mean(c01), mean_c02 = mean(c02), mean_c03 = mean(c03), mean_c04 = mean(c04),
  mean_c05 = mean(c05), mean_c06 = mean(c06), mean_c07 = mean(c07), mean_c08 = mean(c08), mean_c09 = mean(c09), mean_c09 = mean(c09),
  mean_c10 = mean(c10), mean_c11 = mean(c11) ,mean_c12 = mean(c12), sd_c01 = sd(c01), sd_c02 = sd(c02), sd_c03 = sd(c03), sd_c04 = sd(c04), sd_c05 = sd(c05), sd_c06 = sd(c06), sd_c07 = sd(c07), sd_c08 = sd(c08), sd_c09 = sd(c09), sd_c09 = sd(c09), sd_c10 = sd(c10), sd_c11 = sd(c11), sd_c12 = sd(c12), mean_C = mean(C, na.rm=TRUE), sd_C = sd(C), mean_C2 = mean(`C#|Db`, na.rm=TRUE), mean_D = mean(D, na.rm=TRUE), mean_D2 = mean(`D#|Eb`, na.rm=TRUE), mean_E = mean(E, na.rm=TRUE), mean_F = mean(F, na.rm=TRUE), mean_F2 = mean(`F#|Gb`, na.rm=TRUE), mean_G = mean(G, na.rm=TRUE), mean_G2 = mean(`G#|Ab`, na.rm=TRUE), mean_A = mean(A, na.rm=TRUE), mean_A2 = mean(`A#|Bb`, na.rm=TRUE), mean_B = mean(B, na.rm=TRUE), sd_C2 = sd(`C#|Db`, na.rm=TRUE), sd_D = sd(D, na.rm=TRUE), sd_D2 = sd(`D#|Eb`, na.rm=TRUE), sd_E = sd(E, na.rm=TRUE), sd_F = sd(F, na.rm=TRUE), sd_F2 = sd(`F#|Gb`, na.rm=TRUE), sd_G = sd(G, na.rm=TRUE), sd_G2 = sd(`G#|Ab`, na.rm=TRUE), sd_A = sd(A, na.rm=TRUE), sd_A2 = sd(`A#|Bb`, na.rm=TRUE), sd_B = sd(B, na.rm=TRUE))
```


```{r, echo=FALSE}
cluster3_juice <-
  recipe(
    track.album.name ~
     mean_tempo + mean_energy + mean_valence + mean_c01 + mean_c02 + mean_c03 + mean_c04 + mean_c05 + mean_c06 + mean_c07 + mean_c08 + mean_c09 + mean_c10 + mean_c11 + mean_c12 + sd_c01 + sd_c03 + sd_c04 + sd_c05 + sd_c05 + sd_c06 + sd_c07 + sd_c08 + sd_c09 + sd_c10 + sd_c11 + sd_c12 + mean_C + mean_C2 + mean_D + mean_D2 + mean_E + mean_F + mean_F2 + mean_G + mean_G2 + mean_A + mean_A2 + mean_B + sd_C + sd_C2 + sd_D + sd_D2 + sd_E + sd_F + sd_F2 + sd_G + sd_G2 + sd_A + sd_A2,
    data = cluster_test3
  ) %>%
  #step_center(all_predictors()) %>%
  #step_scale(all_predictors()) %>% 
  step_range(all_predictors()) %>% 
  prep(cluster_test3 %>% mutate(track.album.name = str_trunc(track.album.name, 30))) %>%
  juice() %>%
  column_to_rownames("track.album.name")
```


```{r, echo=FALSE}
cluster3_dist <- dist(cluster3_juice, method = "manhattan")
```

```{r, echo=FALSE}
cluster3 <- cluster3_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram(rotate = TRUE) +
  labs(x = "", y = "", title = "Clustering albums on valence, energy, tempo, pitch class and timbre") 
cluster3
```

***

We also saw in the previous tabs that we could include Spotify's lower level features as variables. When adding the average timbre coefficients and average pitch classes for each album to the clustering algorithm, we are obtaining completely different clusters compared to the clusters created when just using the higher level features.

For our first of the two main clusters, we can see that only the first two albums of the *'Higher Education'* trilogy remain together, and that his 3rd album *'Graduation'* is now moved to a cluster quite far from the other two. Taken its place, we have his 8th album *'ye'* which sounds nothing like the first two albums. 

For all subclusters, there is not much of an underlying theme and I feel like the previous clusters were more representative. 

### Conclusion

The purpose of this project was to find possible clusters of albums for the discography of Kanye West. Using Spotify's API we found suitable features like valence, tempo and energy. Using these features, we found suitable clusters which distinguished his early work from his newer work, whilst his latest album actually belonged to this early cluster. This indicates that Kanye West might be returning to his older self.

We also saw that Spotify's lower level features such as timbre coefficients and pitch classes could be used to for a clustering algorithm. When including these to the clustering algorithm, we actually found clusters that did not make much sense. This is probably because whilst there are some differences between albums for these features, they are not representative for the albums' sound. 

Concluding, we were able to find an **Old** Kanye West and a **New** Kanye West. These labels are mostly related to the emotional level and the tempo of his music. As his latest album can be seen as *Old Kanye*, I'm eager to find out if his upcoming work is keeping up with this trend, or if we are entering a completely new era of Kanye West. 
