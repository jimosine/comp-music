---
title: "Computational Musicology 2020-2021"
author: "Jim Buissink"
#date: "2/9/2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(spotifyr)
library(tidyverse)
library(knitr)
library(ggthemes)
library(plotly)
library(compmus)
```

### Introduction

> I miss the old Kanye... 
> - Kanye West (2016)

But who, what & when is this 'old' Kanye? Is it possible to uncover who the old Kanye is by analyzing his music? Are there any significant differences in his earlier and later works, and is there a way to cluster these works so we have an 'old' Kanye and a 'new' Kanye? 

To find an answer to these questions, we will build a corpus consisting of the 9 studio albums released by Kanye West and his 2 collaborative albums. Any non-musical tracks, e.g. skits, featured on the albums will be included. If a distinction can be made between old Kanye and new Kanye, we expect there to be musical differences between his first three albums and his latter releases. This is where the music changed from cheery & soulful to dark & electronic-inspired, so we expect to see differences in valence. His latest album, Jesus Is King (2019), is an outlier concerning this stylistic change and could mean that Kanye West is returning to his original sound, or that potentially a 'future' Kanye is in the making.

The 2 collaborative albums (with Jay-Z and Kid Cudi, respectivly) are included as I feel that Kanye did have enough of an influence during the creation of these works so his personality should be reflected in the music, although it might be for only 50%. Singles and tracks where Kanye was a featured artist are excluded from the corpus, but might serve as a reference in a later stages.

### Distribution of Valence 
```{r, echo=FALSE}
kanye_albums <- get_playlist_audio_features("", "4bQNKK5gntETwqlI1RW9w1")


#ADD MEAN FEATURES
kanye_albums  <- transform(kanye_albums,
    mean_tempo = mean(tempo),
    mean_valence = mean(valence),
    mean_speechiness = mean(speechiness),
    mean_energy = mean(energy)

)


#ADD POSITIVE COLUMN
kanye_albums <- kanye_albums %>% add_column(mood = 'yes')
kanye_albums <- kanye_albums %>% 
  mutate(mood= ifelse(track.album.name == 'The College Dropout' | track.album.name == 'Graduation' | track.album.name == 'Late Registration', "Positive", "Negative"))

#ADD AVG Tempo Column
kanye_albums <- kanye_albums %>% 
  mutate(avg_tempo= ifelse(track.album.name == 'The College Dropout' | track.album.name == 'JESUS IS KING' | track.album.name == 'Graduation' | track.album.name == 'Late Registration', "< 100 bpm", "> 100 bpm"))


```

```{r, echo=FALSE}

#BOXPLOTS 1
p <- ggplot(kanye_albums, aes(x = reorder(track.album.release_date, desc(track.album.release_date)), y = valence, fill=mood)) + 
  geom_boxplot() 
p <- p + scale_x_discrete(labels=c("JESUS IS KING","KIDS SEE GHOSTS","ye","The Life Of Pablo", "Yeezus", "Watch The Throne", "My Beautiful Dark Twisted Fantasy", "808s & Heartbreak", "Graduation", "Late Registration", "The College Dropout"))
p <- p +  geom_hline(yintercept = 0.40, linetype = 3)
p <- p + coord_flip() + labs(title="Distribution of the valence feature",y="", x = "")

p <- p + annotate(
  "text",
  x = 2.5, y = 0.58,
  label = "Average\nvalence",
  vjust = 1, size = 3, color = "grey40"
)

p <- p + annotate(
  "curve",
  x = 2.5, y = 0.58,
  xend = 2.5, yend = 0.40,
  arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
  color = "grey40")
p <- p + theme(legend.title=element_blank())
p #+ theme_economist_white()
```

***

Before we can cluster the works in to two (or potentially three) groups, we need to find out which audio features contain the most useful information.

As already was mentioned in the previous section, valence seems to be a good indicator. Below is a plot of the distribution of the valence for each album. We can see that his first three albums sound positive (valence > 0.5) and the latter sound negative (valence < 0.5)

### Distribution of Tempo 
```{r, echo=FALSE}
#BOXPLOTS 2
p2 <- ggplot(kanye_albums, aes(x = reorder(track.album.release_date, desc(track.album.release_date)), y = tempo, fill=avg_tempo)) + 
  geom_boxplot() 
p2 <- p2 + scale_x_discrete(labels=c("JESUS IS KING","KIDS SEE GHOSTS","ye","The Life Of Pablo", "Yeezus", "Watch The Throne", "My Beautiful Dark Twisted Fantasy", "808s & Heartbreak", "Graduation", "Late Registration", "The College Dropout"))
p2 <- p2 +  geom_hline(yintercept = 112, linetype = 3)
p2 <- p2 +  coord_flip() + labs(title="Distribution of the tempo feature",y="", x = "")

p2 <- p2 + annotate(
  "text",
  x = 10, y = 135,
  label = "Average\ntempo",
  vjust = 1, size = 3, color = "grey40"
)

p2 <- p2 + annotate(
  "curve",
  x = 10, y = 135,
  xend = 9, yend = 112,
  arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
  color = "grey40")

p2 <- p2 + theme(legend.title=element_blank())
p2

```

***

Another feature with similar behavior is the tempo. The BPM used for hip-hop beats is generally around 60-100 bpm. We can see from the plot blow that only his first three albums have a tempo < 100, with the an exception for his latest album 'JESUS IS KING'.


### Emotion

```{r , echo=FALSE}
mood <- kanye_albums %>% ggplot(aes(x = valence, y = energy, color = track.album.release_date)) +
  geom_jitter(alpha = 0.6) + geom_hline(yintercept = 0.5) + geom_vline(xintercept = 0.5) 

mood <- mood + labs(title="Distribution of tracks according to valence and energy", colour = "Release date")

mood <- mood + annotate(
  "text",
  x = 0.90, y = 0.95,
  label = "HAPPY",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.1, y = 0.95,
  label = "ANGRY",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.1, y = 0.1,
  label = "SAD",
  vjust = 1, size = 4
)

mood <- mood + annotate(
  "text",
  x = 0.90, y = 0.1,
  label = "CALM",
  vjust = 1, size = 4
)
fig <- ggplotly(mood)

fig
```

***

Whereas valence as a standalone feature tells us something about the general mood of a song, e.g. positive or negative, we could combine the valence with the energy feature. This pair of parameters now tells us something about the emotional level of a track. Low energy + low valence songs are considered 'sad', low valence + high energy are 'angry', songs with high valence + low energy are 'calm' and high valence + high energy are 'happy'. This section needs some more work.

### Chromagram

```{r, echo=FALSE}
angriest <-
  get_tidy_audio_analysis("46fk9wjYcPm0sgym2b7EEE") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```

```{r, echo=FALSE}
#`manhattan`, `euclidean`, or `chebyshev`
angriest %>%
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

*** 

Let's have a look at the most angriest track in Kanye's discography: "We Major" from the album "Late Regristration". This album itself is the second most happiest album (looking at mean Valence + mean Energy) which makes this track a significant outlier. Upon listening to the the track, I don't get a particular angry vibe from it. Let's find out what is happening in the track by expecting its chromagram.

From the chromagram, we can see a lot of movement in the track but a clear break happening at the 300 seconds mark. During this break/bridge, we can see that everything is F#/G flat. At this part, the music doesn't really change that much but we have Kanye West speak in an aggressive tone, clearly different from the rest